{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from data_loader import get_loader\n",
    "import time\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, embedding_size, vocab_size, number_classes):\n",
    "        super(Model, self).__init__()\n",
    "        self.embedding_size = embedding_size # embedding space dimension\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.linear = nn.Linear(embedding_size, number_classes)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        batch_size = input_tensor.shape[0]\n",
    "        input_tensor = input_tensor.permute(1, 0) # seq_len * batch_size\n",
    "        x = self.embedding(input_tensor) # seq_len * batch_size * embedding_size\n",
    "        x = torch.mean(x, 0) # batch_size * embedding_size\n",
    "        logits = self.linear(x) # batch_size * number_classes\n",
    "        return self.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainOneBatch(model, batch_input, optimizer, criterion):\n",
    "    optimizer.zero_grad()\n",
    "    sequences = batch_input[0] # get input sequence of shape: batch_size * sequence_len\n",
    "    targets = batch_input[1] # get targets of shape : batch_size\n",
    "    out = model.forward(sequences) # shape: batch_size * number_classes \n",
    "    loss = criterion(out, targets)\n",
    "    loss.backward() # compute the gradient\n",
    "    optimizer.step() # update network parameters\n",
    "    return loss.item() # return loss value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader):\n",
    "    count_batch = 0\n",
    "    accuracy = 0\n",
    "    for batch in data_loader:\n",
    "        sequences = batch[0]\n",
    "        target = batch[1]\n",
    "        out, = model.forward(sequences)\n",
    "        predicted = torch.argmax(out, -1)\n",
    "        accuracy += torch.sum(predicted==target).item()/(sequences.shape[0])\n",
    "        count_batch += 1\n",
    "    accuracy = accuracy/count_batch\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(model, path_documents, path_labels, word2ind, n_epochs=5, batch_size=16,  printEvery=20):\n",
    "    data_loader_params = (path_documents, path_labels, word2ind, str(device), batch_size)\n",
    "    epoch = 0\n",
    "    loss = 0\n",
    "    count_iter = 0\n",
    "    patience = 0 # before interrupting training \n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    #negative log likelihood\n",
    "    criterion = nn.NLLLoss()\n",
    "    time1 = time.time()\n",
    "    training_accuracy_epochs = [] # save training accuracy for each epoch\n",
    "    validation_accuracy_epochs = [] # save validation accuracy for each epoch \n",
    "    for i in range(n_epochs):\n",
    "        loader = get_loader(*data_loader_params)\n",
    "        for batch in loader:\n",
    "            loss += trainOneBatch(model, batch, optimizer, criterion)\n",
    "            count_iter += 1\n",
    "            if count_iter % printEvery == 0:\n",
    "                time2 = time.time()\n",
    "                print(\"Iteration: {0}, Time: {1:.4f} s, training loss: {2:.4f}\".format(count_iter,\n",
    "                                                                          time2 - time1, loss/printEvery))\n",
    "                loss = 0\n",
    "        training_accuracy = evaluate(model, get_loader(*data_loader_params))\n",
    "        validation_accuracy = evaluate(model, get_loader(*data_loader_params))\n",
    "        print('Epoch {0} done: training_accuracy = {1:.3f}, validation_accuracy = {2:.3f}'.format(i+1, training_accuracy, validation_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_cat2ind = 'data/cat2ind.csv'\n",
    "path_word_count = 'data/word2count.txt'\n",
    "\n",
    "#load index to category mapping\n",
    "ind2category = {}\n",
    "word2ind = {'PAD':0, 'OOV':1}\n",
    "with open(path_cat2ind, encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        mapping = line.split(',')\n",
    "        ind2category[int(mapping[1])] = mapping[0]\n",
    "\n",
    "#load word to index mapping\n",
    "count = 2\n",
    "with open(path_word_count) as f:\n",
    "    for line in f:\n",
    "        mapping = line.split('\\t')\n",
    "        word2ind[mapping[0]] = count\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = Model(50, len(word2ind), len(ind2category)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'M' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-ea9e3be850bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpath_documents_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'data/train_documents.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpath_labels_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'data/train_labels.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrainModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_documents_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_labels_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2ind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-30-da7d282e9904>\u001b[0m in \u001b[0;36mtrainModel\u001b[0;34m(model, path_documents, path_labels, word2ind, n_epochs, batch_size, printEvery)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdata_loader_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrainOneBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mcount_iter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcount_iter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mprintEvery\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-7afe880fd596>\u001b[0m in \u001b[0;36mtrainOneBatch\u001b[0;34m(model, batch_input, optimizer, criterion)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# get input sequence of shape: batch_size * sequence_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# get targets of shape : batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# shape: batch_size * number_classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# compute the gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-ee3188596ad2>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_tensor)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# seq_len * batch_size * embedding_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# batch_size * embedding_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# batch_size * number_classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'M' is not defined"
     ]
    }
   ],
   "source": [
    "path_documents_train = 'data/train_documents.txt'\n",
    "path_labels_train = 'data/train_labels.txt'\n",
    "trainModel(my_model, path_documents_train, path_labels_train, word2ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
