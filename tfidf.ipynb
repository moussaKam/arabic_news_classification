{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Tfidf(object):\n",
    "    def __init__(self, collection, stop_words=[], min_count=3 ,max_vocab=50000):\n",
    "        '''collection is a list of strings'''\n",
    "        self.word2count = {} # map each word to its total number of occurences in the whole collection\n",
    "        self.word2count_doc = {} # map each word to the number of documents in which it appears\n",
    "        self.word2ind = {} # map each word in the collection to a unique index\n",
    "        self.ind2word = {}\n",
    "        self.word2idf = {} # map each word to its idf\n",
    "        self.collection = collection\n",
    "        self.documents = [document.split() for document in collection]\n",
    "        \n",
    "        for document in self.documents:\n",
    "            document_unique_words = set()\n",
    "            for word in document:\n",
    "                document_unique_words.add(word)\n",
    "                if word in self.word2count:\n",
    "                    self.word2count[word] += 1\n",
    "                else:\n",
    "                    self.word2count[word] = 1\n",
    "            for word in document_unique_words:\n",
    "                if word in self.word2count_doc:\n",
    "                    self.word2count_doc[word] += 1\n",
    "                else:\n",
    "                    self.word2count_doc[word] = 1\n",
    "                  \n",
    "        self.word2count = {el[0]:el[1] for el in sorted(self.word2count.items(), key=lambda item: item[1],\n",
    "                                                        reverse=True)} # sort by values (descending)\n",
    "        self.unique_words = list(self.word2count.keys())[:max_vocab]\n",
    "        self.unique_words = [word for word in self.unique_words if word not in stop_words \\\n",
    "                             and self.word2count[word] > min_count] # remove stopwords\n",
    "        self.count_unique_words = len(self.unique_words)\n",
    "        self.word2ind = dict(zip(self.unique_words,range(self.count_unique_words)))\n",
    "        self.ind2word = {v:k for k,v in self.word2ind.items()}\n",
    "        self.count_documents = len(collection)        \n",
    "        \n",
    "        # compute the idf of unqiue words in the collection\n",
    "        for word in self.word2ind.keys():\n",
    "            count = self.word2count_doc[word]\n",
    "            idf = np.log(self.count_documents / (1 + count)) + 1 \n",
    "            self.word2idf[word] = idf\n",
    "\n",
    "    def getWordFromInd(self, ind):\n",
    "        return self.ind2word[ind]\n",
    "\n",
    "    def getListWords(self):\n",
    "        return self.unique_words\n",
    "\n",
    "    def tf(self, document):\n",
    "        '''\n",
    "        return the frequency of each unique word in document\n",
    "        document is a list of strings\n",
    "        '''\n",
    "        word2frequency = {}\n",
    "        for word in document:\n",
    "            word2frequency[word] = word2frequency.get(word, 0) + 1 # increment, creating key if it doesn't already exist\n",
    "        return word2frequency\n",
    "\n",
    "    def transform(self, collection):\n",
    "        documents = [document.split() for document in collection] # tokenize documents in the collection\n",
    "        tfidf_mat = np.zeros((len(documents), self.count_unique_words)) # intialize tfidf matrix with zeros\n",
    "        # compute tfidf \n",
    "        for ind, document in enumerate(documents):\n",
    "            word2frequency = self.tf(document)\n",
    "            for word in word2frequency.keys():\n",
    "                if word in self.word2ind:\n",
    "                    tfidf_mat[ind, self.word2ind[word]] = word2frequency[word] * self.word2idf[word]        \n",
    "        return tfidf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_path = 'data'\n",
    "\n",
    "def read_data(subset):\n",
    "    with open(os.path.join(data_path, '{}_documents.txt'.format(subset)), 'r') as f1, \\\n",
    "        open(os.path.join(data_path, '{}_labels.txt'.format(subset)), 'r') as f2:\n",
    "        documents = []\n",
    "        labels = []\n",
    "        for line in f1:\n",
    "            documents.append(line)\n",
    "        f1.close()\n",
    "        for line in f2:\n",
    "            labels.append(int(line))\n",
    "        return documents, labels\n",
    "              \n",
    "\n",
    "collection_train, labels_train = read_data('train')\n",
    "collection_valid, labels_valid = read_data('valid')\n",
    "collection_test, labels_test = read_data('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = Tfidf(collection_train)\n",
    "X_train = tfidf.transform(collection_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/moussa/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=200,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(multi_class='auto', solver='lbfgs', max_iter=200) \n",
    "classifier.fit(X_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = tfidf.transform(collection_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid_predicted = classifier.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9406\n"
     ]
    }
   ],
   "source": [
    "print(sum(y_valid_predicted == labels_valid) / len(labels_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = y_valid_predicted == labels_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [i for i, x in enumerate(list(l)) if x == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6,\n",
       " 8,\n",
       " 18,\n",
       " 40,\n",
       " 51,\n",
       " 86,\n",
       " 105,\n",
       " 167,\n",
       " 171,\n",
       " 221,\n",
       " 248,\n",
       " 276,\n",
       " 286,\n",
       " 289,\n",
       " 293,\n",
       " 331,\n",
       " 366,\n",
       " 373,\n",
       " 382,\n",
       " 396,\n",
       " 426,\n",
       " 428,\n",
       " 460,\n",
       " 464,\n",
       " 467,\n",
       " 475,\n",
       " 483,\n",
       " 494,\n",
       " 511,\n",
       " 515,\n",
       " 526,\n",
       " 556,\n",
       " 568,\n",
       " 571,\n",
       " 577,\n",
       " 581,\n",
       " 672,\n",
       " 686,\n",
       " 694,\n",
       " 723,\n",
       " 730,\n",
       " 736,\n",
       " 762,\n",
       " 771,\n",
       " 783,\n",
       " 786,\n",
       " 791,\n",
       " 792,\n",
       " 801,\n",
       " 813,\n",
       " 846,\n",
       " 856,\n",
       " 862,\n",
       " 869,\n",
       " 891,\n",
       " 899,\n",
       " 904,\n",
       " 914,\n",
       " 923,\n",
       " 924,\n",
       " 935,\n",
       " 994,\n",
       " 1050,\n",
       " 1068,\n",
       " 1075,\n",
       " 1092,\n",
       " 1124,\n",
       " 1126,\n",
       " 1130,\n",
       " 1132,\n",
       " 1167,\n",
       " 1176,\n",
       " 1236,\n",
       " 1237,\n",
       " 1257,\n",
       " 1281,\n",
       " 1283,\n",
       " 1311,\n",
       " 1325,\n",
       " 1359,\n",
       " 1366,\n",
       " 1385,\n",
       " 1395,\n",
       " 1421,\n",
       " 1428,\n",
       " 1434,\n",
       " 1457,\n",
       " 1518,\n",
       " 1523,\n",
       " 1550,\n",
       " 1572,\n",
       " 1579,\n",
       " 1599,\n",
       " 1607,\n",
       " 1612,\n",
       " 1634,\n",
       " 1641,\n",
       " 1644,\n",
       " 1651,\n",
       " 1656,\n",
       " 1671,\n",
       " 1679,\n",
       " 1693,\n",
       " 1699,\n",
       " 1803,\n",
       " 1804,\n",
       " 1810,\n",
       " 1827,\n",
       " 1836,\n",
       " 1857,\n",
       " 1866,\n",
       " 1877,\n",
       " 1880,\n",
       " 1908,\n",
       " 1925,\n",
       " 1926,\n",
       " 1952,\n",
       " 1958,\n",
       " 1969,\n",
       " 2000,\n",
       " 2004,\n",
       " 2023,\n",
       " 2055,\n",
       " 2111,\n",
       " 2141,\n",
       " 2162,\n",
       " 2177,\n",
       " 2190,\n",
       " 2206,\n",
       " 2233,\n",
       " 2238,\n",
       " 2301,\n",
       " 2307,\n",
       " 2308,\n",
       " 2318,\n",
       " 2327,\n",
       " 2344,\n",
       " 2353,\n",
       " 2373,\n",
       " 2380,\n",
       " 2389,\n",
       " 2421,\n",
       " 2478,\n",
       " 2489,\n",
       " 2490,\n",
       " 2497,\n",
       " 2520,\n",
       " 2561,\n",
       " 2590,\n",
       " 2625,\n",
       " 2643,\n",
       " 2679,\n",
       " 2689,\n",
       " 2714,\n",
       " 2793,\n",
       " 2816,\n",
       " 2826,\n",
       " 2828,\n",
       " 2835,\n",
       " 2838,\n",
       " 2853,\n",
       " 2860,\n",
       " 2880,\n",
       " 2882,\n",
       " 2884,\n",
       " 2892,\n",
       " 2894,\n",
       " 2911,\n",
       " 2930,\n",
       " 2947,\n",
       " 2969,\n",
       " 2989,\n",
       " 3040,\n",
       " 3043,\n",
       " 3058,\n",
       " 3066,\n",
       " 3081,\n",
       " 3094,\n",
       " 3098,\n",
       " 3107,\n",
       " 3112,\n",
       " 3113,\n",
       " 3120,\n",
       " 3140,\n",
       " 3144,\n",
       " 3189,\n",
       " 3190,\n",
       " 3206,\n",
       " 3210,\n",
       " 3214,\n",
       " 3218,\n",
       " 3249,\n",
       " 3264,\n",
       " 3280,\n",
       " 3282,\n",
       " 3292,\n",
       " 3300,\n",
       " 3315,\n",
       " 3366,\n",
       " 3420,\n",
       " 3427,\n",
       " 3436,\n",
       " 3437,\n",
       " 3479,\n",
       " 3531,\n",
       " 3538,\n",
       " 3556,\n",
       " 3568,\n",
       " 3598,\n",
       " 3599,\n",
       " 3611,\n",
       " 3642,\n",
       " 3644,\n",
       " 3648,\n",
       " 3665,\n",
       " 3666,\n",
       " 3693,\n",
       " 3704,\n",
       " 3736,\n",
       " 3740,\n",
       " 3786,\n",
       " 3806,\n",
       " 3811,\n",
       " 3841,\n",
       " 3852,\n",
       " 3864,\n",
       " 3866,\n",
       " 3880,\n",
       " 3891,\n",
       " 3901,\n",
       " 3903,\n",
       " 3912,\n",
       " 3927,\n",
       " 3940,\n",
       " 3957,\n",
       " 3971,\n",
       " 3983,\n",
       " 3984,\n",
       " 4000,\n",
       " 4031,\n",
       " 4076,\n",
       " 4148,\n",
       " 4181,\n",
       " 4190,\n",
       " 4202,\n",
       " 4217,\n",
       " 4257,\n",
       " 4259,\n",
       " 4289,\n",
       " 4310,\n",
       " 4322,\n",
       " 4328,\n",
       " 4347,\n",
       " 4374,\n",
       " 4377,\n",
       " 4381,\n",
       " 4382,\n",
       " 4389,\n",
       " 4397,\n",
       " 4415,\n",
       " 4435,\n",
       " 4438,\n",
       " 4443,\n",
       " 4475,\n",
       " 4486,\n",
       " 4487,\n",
       " 4528,\n",
       " 4534,\n",
       " 4554,\n",
       " 4558,\n",
       " 4560,\n",
       " 4573,\n",
       " 4584,\n",
       " 4595,\n",
       " 4605,\n",
       " 4639,\n",
       " 4649,\n",
       " 4660,\n",
       " 4664,\n",
       " 4676,\n",
       " 4688,\n",
       " 4697,\n",
       " 4699,\n",
       " 4717,\n",
       " 4724,\n",
       " 4731,\n",
       " 4740,\n",
       " 4762,\n",
       " 4815,\n",
       " 4835,\n",
       " 4871,\n",
       " 4873,\n",
       " 4893,\n",
       " 4920,\n",
       " 4922,\n",
       " 4952,\n",
       " 4990]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 4)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_valid[18], y_valid_predicted[18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  افادت السفارة البريطانية في بيان ، ان \" الطلاب الفلسطينيين في لبنان سيتمكنون من الاستفادة من دعم جديد تقدمه المملكة المتحدة للمساهمة في مساعدة الاونروا على ابقاء مدارسها مفتوحة للعام الدراسي 2015/2016 \".  واعلن وزير التنمية الدولية البريطاني ديسموند سواين ان بلاده ستقدم دعما اضافيا بقيمة 3 ملايين جنيه استرليني للاونروا لكي يتسنى لكل طالب فلسطيني ان يذهب الى المدرسة .  واضاف البيان ان هذا الدعم يساعد \" الاونروا \" في تغطية العجز الذي لم تشهده المؤسسة من قبل ، وقيمته 101 مليون دولار اميركي ، وفي اعادة فتح 685 مدرسة مع بداية العام الدراسي . وتقدم \" الاونروا \" خدمات تعليمية اساسية لنصف مليون لاجئ فلسطيني في لبنان ، والاردن ، وسوريا ، وقطاع غزة والضفة الغربية . يذكر ان وزارة التنمية الدولية البريطانية هي ثالث اكبر ممول لميزانية الانروا العامة وميزانية الطوارئ .  وفي هذا السياق قال سواين : \" تلتزم المملكة المتحدة دعم اللاجئين الفلسطينيين عبر الاونروا ، ونعتبر التعليم اولوية مهمة . وتضطلع الاونروا بدور اساسي في منطقة غير مستقرة حيث توفر الخدمات الاساسية والمساعدات الانسانية . كما توفر بيئة امنة للاطفال للتعلم واللعب ، مما يوسع رؤيتهم للعالم من خلال دروس تتناول حقوق الانسان والتسامح ، وتعطيهم المهارات لكسب عيشهم وتامين مدخولهم . يسرني ان وزارة التنمية الدولية البريطانية تدعم هذا العمل ، فدعمنا الذي يبلغ قيمته 3 ملايين جنيه استرليني سيضمن استمرار طلاب المدارس الذهاب الى المدرسة . وقد بلغت بذلك مساعدة وزارة التنمية الدولية للاونروا منذ بداية العام 1943 مليون جنيه استرليني \".  واضاف : \" نحن نرحب بالجهود المشتركة للمولين بمن فيهم اولئك في المنطقة من اجل تغطية عجز هذا العام ، علما ان هذا العجز مزمن ولا يمكن معالجته بتوفير التمويل فحسب ، وانما على الاونروا ان تتابع اصلاحات مالية من اجل معالجته لتقديم الخدمات الاساسية للاجئيين للفلسطينيين وخفض المصاريف التشغيلية \". \\n'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection_valid[18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
