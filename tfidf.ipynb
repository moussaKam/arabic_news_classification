{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words \n",
    "\n",
    "Documents are traditionally represented with the vector space model, also known as the bag-of-words representation.\n",
    "\n",
    "With this approach, each document $d_{i}$ from the collection $D=\\{d_{1} \\dots d_{m}\\}$ of size $m$ is associated with an $n$-dimensional feature vector, where $n$ is the number of unique terms in the preprocessed collection.\n",
    "The set of unique terms $T=\\{t_{1} \\dots t_{n}\\}$ is called the vocabulary.\n",
    "\n",
    "## TF-IDF\n",
    "\n",
    "Term Frequency-Inverse Document Frequency (TF-IDF), is a method used to compute the coordinates of a document in the vector space.\n",
    "\n",
    "The assumption is that the importance of a word to a document increases when its frequency increases.\n",
    "However, considering the frequency as the only factor to judge the importance of a word would result in giving greater weight to commonly used terms such as stopwords, which could be misleading in many tasks.\n",
    "TF-IDF mitigates this problem by introducing a factor that diminishes the weight of words that occur frequently in other documents in the same collection.\n",
    "The TF-IDF weight computation is based on the product of two separate factors, namely the Term Frequency (TF) and the Inverse Document Frequency (IDF).\n",
    "More specifically:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathrm{weight}(t,d,D) = tf(t,d) \\times idf(t,D)\n",
    "\\end{equation}\n",
    "\n",
    "There are many ways to determine $tf$ and $idf$. In our case, $tf(t,d)$ is the number of times term $t$ appears in document $d$, and $idf(t,D) = \\ln \\big( \\frac{m}{1+df(t)} \\big) + 1 $, with $df(t)$ the number of documents in the collection $D$ that contains $t$.\n",
    "We can notice that the weight of a term in a document increases when its frequency increases in this document (first factor), and decreases when the number of documents in the collection containing this term increases (second factor).\n",
    "$M \\in \\mathbb{R}^{m \\times n}$ is the TF-IDF matrix of $D$, where $M_{ij}$ is the TF-IDF weight of the $jth$ word in the $ith$ document.\n",
    "\n",
    "In the following we provide a simple implementation of this approach using numpy only. Note that we can use scikit-learn to compute the TF-IDF matrix, however we prefered to implement it ourselves for educational purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When initialized, a _tfidf_ object takes as input a collection of documents and computes the IDF of words in this collection.\n",
    "We can provide a list of stopwords to be ignored, and we can specify the minimun number of occurences of a word allowing it to be included in our vocabulary. And finally we can choose a maximum size of the vocabulary. \n",
    "\n",
    "Two important methods are defined in this class: _tf_ that returns the frequency of unique words in a document and _transform_ that returns the TF-IDF matrix of the collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "\n",
    "class Tfidf(object):\n",
    "    def __init__(self, collection, stop_words=[], min_count=3 ,max_vocab=50000):\n",
    "        '''collection is a list of strings'''\n",
    "        self.word2count = {} # map each word to its total number of occurences in the whole collection\n",
    "        self.word2count_doc = {} # map each word to the number of documents in which it appears\n",
    "        self.word2ind = {} # map each word in the collection to a unique index\n",
    "        self.ind2word = {}\n",
    "        self.word2idf = {} # map each word to its idf\n",
    "        self.collection = collection\n",
    "        self.documents = [document.split() for document in collection]\n",
    "        \n",
    "        for document in self.documents:\n",
    "            document_unique_words = set()\n",
    "            for word in document:\n",
    "                document_unique_words.add(word)\n",
    "                self.word2count[word] = self.word2count.get(word, 0) + 1\n",
    "            for word in document_unique_words:\n",
    "                self.word2count_doc[word] = self.word2count_doc.get(word, 0) + 1\n",
    "                  \n",
    "        self.word2count = {el[0]:el[1] for el in sorted(self.word2count.items(), key=lambda item: item[1],\n",
    "                                                        reverse=True)} # sort by values (descending)\n",
    "        self.unique_words = [word for word in self.word2count.keys() if word not in stop_words \\\n",
    "                             and self.word2count[word] > min_count] # remove stopwords\n",
    "        self.unique_words = list(self.unique_words)[:max_vocab]\n",
    "        self.count_unique_words = len(self.unique_words)\n",
    "        self.word2ind = dict(zip(self.unique_words,range(self.count_unique_words)))\n",
    "        self.ind2word = {v:k for k,v in self.word2ind.items()}\n",
    "        self.count_documents = len(collection)        \n",
    "        \n",
    "        # compute the idf of unqiue words in the collection\n",
    "        for word in self.word2ind.keys():\n",
    "            count = self.word2count_doc[word]\n",
    "            idf = np.log(self.count_documents / (1 + count)) + 1 \n",
    "            self.word2idf[word] = idf\n",
    "\n",
    "    def getWordFromInd(self, ind):\n",
    "        return self.ind2word[ind]\n",
    "\n",
    "    def getListWords(self):\n",
    "        return self.unique_words\n",
    "\n",
    "    def tf(self, document):\n",
    "        '''\n",
    "        return the frequency of each unique word in document\n",
    "        document is a list of strings\n",
    "        '''\n",
    "        word2frequency = {}\n",
    "        for word in document:\n",
    "            word2frequency[word] = word2frequency.get(word, 0) + 1 # increment, creating key if it doesn't already exist\n",
    "        return word2frequency\n",
    "\n",
    "    def transform(self, collection):\n",
    "        '''\n",
    "        returns the tfidf matrix of a collection of documents of dimension n_doc * vocab_size\n",
    "        collection is a list of strings\n",
    "        '''\n",
    "        documents = [document.split() for document in collection] # tokenize documents in the collection\n",
    "        tfidf_mat = np.zeros((len(documents), self.count_unique_words)) # intialize tfidf matrix with zeros\n",
    "        # compute tfidf \n",
    "        for ind, document in enumerate(documents):\n",
    "            word2frequency = self.tf(document)\n",
    "            for word in word2frequency.keys():\n",
    "                if word in self.word2ind:\n",
    "                    tfidf_mat[ind, self.word2ind[word]] = word2frequency[word] * self.word2idf[word]        \n",
    "        return tfidf_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Classification\n",
    "\n",
    "In the following we start by computing the TF-IDF matrix of the train set, and we train a simple logistic regression model on this subset using scikit-learn. Then we load the test set and we compute its TF-IDF matrix as well. Note that the documents in the test set are represented in the space made of the unique terms in the training set __only__ (words in the test set absent from the training set are disregarded).\n",
    "\n",
    "The trained model will then be used to predict the classes of the test documents. The predicted labels will finally be compared with the true labels to calculate the accuracy of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_path = 'data'\n",
    "\n",
    "def read_data(subset):\n",
    "    with open(os.path.join(data_path, '{}_documents.txt'.format(subset)), 'r') as f1, \\\n",
    "        open(os.path.join(data_path, '{}_labels.txt'.format(subset)), 'r') as f2:\n",
    "        documents = []\n",
    "        labels = []\n",
    "        for line in f1:\n",
    "            documents.append(line)\n",
    "        f1.close()\n",
    "        for line in f2:\n",
    "            labels.append(int(line))\n",
    "        return documents, labels\n",
    "              \n",
    "\n",
    "collection_train, labels_train = read_data('train')\n",
    "# collection_valid, labels_valid = read_data('valid')\n",
    "collection_test, labels_test = read_data('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = Tfidf(collection_train)\n",
    "X_train = tfidf.transform(collection_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=200,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = LogisticRegression(multi_class='auto', solver='lbfgs', max_iter=200)\n",
    "classifier.fit(X_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 93.72%\n"
     ]
    }
   ],
   "source": [
    "X_test = tfidf.transform(collection_test)\n",
    "y_test_predicted = classifier.predict(X_test)\n",
    "accuracy = (sum(y_test_predicted == labels_test) / len(labels_test)) * 100\n",
    "print('Test accuracy: {}%'.format(accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
