{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we implement a simple (yet efficient) neural network for supervised text classification.\n",
    "\n",
    "The input sequence of words is first passed to an embedding layer. The vectors at the output of the embedding layer are then averged into the text representation vector. Finally the text representation vector is linearly projected to the output vector that is followed by a softmax activation function.\n",
    "\n",
    "<!-- ![Simple_NN](../images/Simple_NN.png \"Simple_NN architecture\") -->\n",
    "<img src=\"images/Simple_NN.png\" width=\"400\"/>\n",
    "\n",
    "__For more details:__ A Joulin, E Grave, P Bojanowski, T Mikolov - arXiv preprint arXiv:1607.01759, 2016\n",
    "https://arxiv.org/pdf/1607.01759.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import sys\n",
    "sys.path += ['../']\n",
    "from data_loader import get_loader\n",
    "import time\n",
    "\n",
    "# for reproducibility\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by implementing the model as described before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, embedding_size, vocab_size, number_classes):\n",
    "        super(Model, self).__init__()\n",
    "        self.embedding_size = embedding_size # embedding space dimension\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.linear = nn.Linear(embedding_size, number_classes)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        batch_size = input_tensor.shape[0]\n",
    "        x = self.embedding(input_tensor) # seq_len * batch_size * embedding_size\n",
    "        x = torch.mean(x, 0) # batch_size * embedding_size\n",
    "        logits = self.linear(x) # batch_size * number_classes\n",
    "        return self.softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we write a function that trains the model by applying one update. This is equivalent to:\n",
    "* One forward propagation. \n",
    "* Computing the gradients through back-propagation. \n",
    "* Updating the weights using a specific optimizer (Adam in this case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainOneBatch(model, batch_input, optimizer, criterion):\n",
    "    optimizer.zero_grad()\n",
    "    sequences = batch_input[0] # get input sequence of shape: batch_size * sequence_len\n",
    "    targets = batch_input[1] # get targets of shape : batch_size\n",
    "    out = model.forward(sequences) # shape: batch_size * number_classes \n",
    "    loss = criterion(out, targets)\n",
    "    loss.backward() # compute the gradient\n",
    "    optimizer.step() # update network parameters\n",
    "    return loss.item() # return loss value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the model we need a function that computes the accuracy of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader):\n",
    "    count_batch = 0\n",
    "    accuracy = 0\n",
    "    for batch in data_loader:\n",
    "        sequences = batch[0]\n",
    "        target = batch[1]\n",
    "        out = model.forward(sequences)\n",
    "        predicted = torch.argmax(out, -1)\n",
    "        accuracy += torch.sum(predicted==target).item()/(sequences.shape[-1])\n",
    "        count_batch += 1\n",
    "    accuracy = accuracy/count_batch\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to train the model. The model will be trained for $n$ epochs. In each epoch a new dataloader is created to generate the mini-batches used in the training.\n",
    "\n",
    "After each epoch, the model is evaluated on the training and the validation set, to verify that no overfitting is occuring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(model, path_documents_train, path_labels_train, path_documents_valid, \n",
    "               path_labels_valid, word2ind, n_epochs=5, batch_size=16,  printEvery=100):\n",
    "    data_loader_train_params = (path_documents_train, path_labels_train, word2ind, str(device), batch_size)\n",
    "    data_loader_valid_params = (path_documents_valid, path_labels_valid, word2ind, str(device), batch_size)\n",
    "    epoch = 0\n",
    "    loss = 0\n",
    "    count_iter = 0\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0015)\n",
    "    #negative log likelihood\n",
    "    criterion = nn.NLLLoss()\n",
    "    time1 = time.time()\n",
    "    training_accuracy_epochs = [] # save training accuracy for each epoch\n",
    "    validation_accuracy_epochs = [] # save validation accuracy for each epoch \n",
    "    for i in range(n_epochs):\n",
    "        loader = get_loader(*data_loader_train_params)\n",
    "        for batch in loader:\n",
    "            loss += trainOneBatch(model, batch, optimizer, criterion)\n",
    "            count_iter += 1\n",
    "            if count_iter % printEvery == 0:\n",
    "                time2 = time.time()\n",
    "                print(\"Iteration: {0}, Time: {1:.4f} s, training loss: {2:.4f}\".format(count_iter,\n",
    "                                                                          time2 - time1, loss/printEvery))\n",
    "                loss = 0\n",
    "        training_accuracy = evaluate(model, get_loader(*data_loader_train_params))\n",
    "        validation_accuracy = evaluate(model, get_loader(*data_loader_valid_params))\n",
    "        print('Epoch {0} done: training_accuracy = {1:.3f}, validation_accuracy = {2:.3f}'.format(i+1, training_accuracy, validation_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_cat2ind = '../data/cat2ind.csv'\n",
    "path_word_count = '../data/word2count.txt'\n",
    "\n",
    "#load index to category mapping\n",
    "ind2category = {}\n",
    "word2ind = {'PAD':0, 'OOV':1}\n",
    "with open(path_cat2ind, encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        mapping = line.split(',')\n",
    "        ind2category[int(mapping[1])] = mapping[0]\n",
    "\n",
    "#load word to index mapping\n",
    "count = 2\n",
    "with open(path_word_count) as f:\n",
    "    for line in f:\n",
    "        mapping = line.split('\\t')\n",
    "        word2ind[mapping[0]] = count\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = Model(50, len(word2ind), len(ind2category)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 300, Time: 1.4657 s, training loss: 1.5861\n",
      "Iteration: 600, Time: 2.7767 s, training loss: 1.4798\n",
      "Iteration: 900, Time: 4.0552 s, training loss: 1.2921\n",
      "Iteration: 1200, Time: 5.3908 s, training loss: 1.0705\n",
      "Iteration: 1500, Time: 6.6976 s, training loss: 0.8903\n",
      "Epoch 1 done: training_accuracy = 0.817, validation_accuracy = 0.789\n",
      "Iteration: 1800, Time: 10.9299 s, training loss: 0.7384\n",
      "Iteration: 2100, Time: 12.2315 s, training loss: 0.6075\n",
      "Iteration: 2400, Time: 13.5311 s, training loss: 0.5347\n",
      "Iteration: 2700, Time: 14.8378 s, training loss: 0.4573\n",
      "Iteration: 3000, Time: 16.1466 s, training loss: 0.4168\n",
      "Epoch 2 done: training_accuracy = 0.932, validation_accuracy = 0.909\n",
      "Iteration: 3300, Time: 20.6058 s, training loss: 0.3819\n",
      "Iteration: 3600, Time: 21.9288 s, training loss: 0.3278\n",
      "Iteration: 3900, Time: 23.2366 s, training loss: 0.3100\n",
      "Iteration: 4200, Time: 24.5049 s, training loss: 0.2787\n",
      "Iteration: 4500, Time: 25.7787 s, training loss: 0.2618\n",
      "Epoch 3 done: training_accuracy = 0.956, validation_accuracy = 0.927\n",
      "Iteration: 4800, Time: 29.8619 s, training loss: 0.2610\n",
      "Iteration: 5100, Time: 31.1605 s, training loss: 0.2302\n",
      "Iteration: 5400, Time: 32.4906 s, training loss: 0.2175\n",
      "Iteration: 5700, Time: 33.7777 s, training loss: 0.1961\n",
      "Iteration: 6000, Time: 35.0781 s, training loss: 0.1910\n",
      "Epoch 4 done: training_accuracy = 0.967, validation_accuracy = 0.932\n",
      "Iteration: 6300, Time: 39.2365 s, training loss: 0.1902\n",
      "Iteration: 6600, Time: 40.5419 s, training loss: 0.1763\n",
      "Iteration: 6900, Time: 41.8372 s, training loss: 0.1605\n",
      "Iteration: 7200, Time: 43.1371 s, training loss: 0.1559\n",
      "Iteration: 7500, Time: 44.4475 s, training loss: 0.1438\n",
      "Iteration: 7800, Time: 45.7926 s, training loss: 0.1477\n",
      "Epoch 5 done: training_accuracy = 0.976, validation_accuracy = 0.937\n",
      "Iteration: 8100, Time: 50.0740 s, training loss: 0.1391\n",
      "Iteration: 8400, Time: 51.4049 s, training loss: 0.1242\n",
      "Iteration: 8700, Time: 52.7383 s, training loss: 0.1249\n",
      "Iteration: 9000, Time: 54.0808 s, training loss: 0.1056\n",
      "Iteration: 9300, Time: 55.4187 s, training loss: 0.1166\n",
      "Epoch 6 done: training_accuracy = 0.982, validation_accuracy = 0.940\n",
      "Iteration: 9600, Time: 59.6080 s, training loss: 0.1125\n",
      "Iteration: 9900, Time: 60.8935 s, training loss: 0.0957\n",
      "Iteration: 10200, Time: 62.1913 s, training loss: 0.0995\n",
      "Iteration: 10500, Time: 63.4807 s, training loss: 0.0853\n",
      "Iteration: 10800, Time: 64.7622 s, training loss: 0.0918\n",
      "Epoch 7 done: training_accuracy = 0.986, validation_accuracy = 0.941\n",
      "Iteration: 11100, Time: 68.9861 s, training loss: 0.0844\n",
      "Iteration: 11400, Time: 70.3267 s, training loss: 0.0823\n",
      "Iteration: 11700, Time: 71.6274 s, training loss: 0.0760\n",
      "Iteration: 12000, Time: 73.0039 s, training loss: 0.0718\n",
      "Iteration: 12300, Time: 74.3459 s, training loss: 0.0676\n",
      "Epoch 8 done: training_accuracy = 0.989, validation_accuracy = 0.941\n",
      "Iteration: 12600, Time: 78.5342 s, training loss: 0.0703\n",
      "Iteration: 12900, Time: 79.8121 s, training loss: 0.0644\n",
      "Iteration: 13200, Time: 81.0902 s, training loss: 0.0592\n",
      "Iteration: 13500, Time: 82.3891 s, training loss: 0.0599\n",
      "Iteration: 13800, Time: 83.6803 s, training loss: 0.0550\n",
      "Epoch 9 done: training_accuracy = 0.992, validation_accuracy = 0.942\n",
      "Iteration: 14100, Time: 87.8240 s, training loss: 0.0559\n",
      "Iteration: 14400, Time: 89.1222 s, training loss: 0.0518\n",
      "Iteration: 14700, Time: 90.4329 s, training loss: 0.0466\n",
      "Iteration: 15000, Time: 91.7346 s, training loss: 0.0484\n",
      "Iteration: 15300, Time: 93.0620 s, training loss: 0.0427\n",
      "Iteration: 15600, Time: 94.3647 s, training loss: 0.0467\n",
      "Epoch 10 done: training_accuracy = 0.993, validation_accuracy = 0.941\n"
     ]
    }
   ],
   "source": [
    "path_documents_train = '../data/train_documents.txt'\n",
    "path_labels_train = '../data/train_labels.txt'\n",
    "path_documents_valid = '../data/valid_documents.txt'\n",
    "path_labels_valid = '../data/valid_labels.txt'\n",
    "trainModel(my_model, path_documents_train, path_labels_train, path_documents_valid,\n",
    "           path_labels_valid, word2ind, n_epochs=10, printEvery=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy = 94.17 %\n"
     ]
    }
   ],
   "source": [
    "path_documents_test = '../data/test_documents.txt'\n",
    "path_labels_test = '../data/test_labels.txt'\n",
    "data_loader_test_params = (path_documents_test, path_labels_test, word2ind, str(device), 10)\n",
    "test_accuracy = evaluate(my_model, get_loader(*data_loader_test_params))\n",
    "print('Test accuracy = {0:.2f} %'.format(test_accuracy * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
